{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "from datetime import date\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For reading in the pickled files\n",
    "\n",
    "subjects_dir = '' # Directory where the pickled user files are located\n",
    "subjects = os.listdir(subjects_dir)\n",
    "subjects_data = []\n",
    "s = []\n",
    "file_names = [] # Specify here the files to read from. Should be comma seperated in list. Removed for privacy.\n",
    "\n",
    "file_map = {index: file_name for index, file_name in enumerate(file_names)}\n",
    "\n",
    "# The following subjects have insufficient amount of data, so ignore them\n",
    "leave_out = [] # Here you can specify which users to leave out from further use\n",
    "\n",
    "# Iterate over file_names and load subject data for valid indices\n",
    "for index, file_name in file_map.items():\n",
    "    if index not in leave_out:\n",
    "        subject_file = os.path.join(subjects_dir, file_name)\n",
    "        # Load subject data from file\n",
    "        with open(subject_file, 'rb') as f:\n",
    "            subject_data = pkl.load(f)\n",
    "            s.append(file_name)\n",
    "            subjects_data.append(subject_data) # Array of data per subject. Index is used as ID of the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make sliding window from array\n",
    "def sliding_window (xs, window_size, stride):\n",
    "    ps = []\n",
    "    if len(xs) <= window_size:\n",
    "        return []\n",
    "    for i in range(len(xs) - window_size + 1):\n",
    "        if i % stride == 0:\n",
    "            ps.append(xs[i: i + window_size])\n",
    "    return ps\n",
    "\n",
    "# Test case\n",
    "print(sliding_window([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4 , 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average the handcrafted features of multiple arrays\n",
    "def handcraft(xs):\n",
    "    s = [0, 0, 0, 0]\n",
    "    n = len(xs)\n",
    "    for i in xs:\n",
    "        h = handcraft_simple(i)\n",
    "        for j in range(4):\n",
    "            s[j] += h[j]\n",
    "    new_s = np.array(s) / n\n",
    "    return new_s\n",
    "\n",
    "# Function to simply create handcrafted features for a single array\n",
    "def handcraft_simple(hr):\n",
    "    min_hr = min(hr)\n",
    "    max_hr = max(hr)\n",
    "    mean_hr = np.mean(hr)\n",
    "    var_hr = np.var(hr)\n",
    "        \n",
    "    return [min_hr, max_hr, mean_hr, var_hr]\n",
    "# Test case\n",
    "print(handcraft([[2,4, 3], [2,6], [2], [2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to get the mean and standard deviation of an array\n",
    "def normalize_vars(xs):\n",
    "    valid_values = xs[~np.isnan(xs)]\n",
    "    mean = np.mean(valid_values)\n",
    "    sd = np.std(valid_values)\n",
    "    return(mean, sd)\n",
    "\n",
    "# Function to normalize array based on mean and standard deviation calculated before.\n",
    "def normalize(xs, mean, sd):\n",
    "    templist = []\n",
    "    for i in range(len(xs)):\n",
    "        templist.append((xs[i] - mean) / sd)\n",
    "    return templist\n",
    "\n",
    "# Test case\n",
    "test1 = np.array([1, 2, 3 ,4, 5, 6])\n",
    "vrs1 = normalize_vars(test1)\n",
    "test2 = np.array([np.NaN, np.NaN, np.NaN, 1, np.NaN, np.NaN])\n",
    "vrs2 = normalize_vars(test2)\n",
    "print(vrs1)\n",
    "print(vrs2)\n",
    "print(normalize(test1, vrs1[0], vrs1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter from 0:00 - 6:00\n",
    "\n",
    "subjects = len(subjects_data) # 54\n",
    "subjects_data_night = []\n",
    "for i in range(subjects):\n",
    "    df = subjects_data[i]\n",
    "    df['time'] = pd.to_datetime(df['time']).dt.time\n",
    "    out = df[df['time'].between(time(0, 0), time(6, 0))]\n",
    "    subjects_data_night.append(out)\n",
    "    \n",
    "print(len(subjects_data_night[0]))\n",
    "print(len((subjects_data[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data into days\n",
    "\n",
    "subjects = len(subjects_data) # 54\n",
    "subjects_data_by_day = []\n",
    "for i in range(subjects): # splits dataframe for each user into days\n",
    "    df = subjects_data[i]\n",
    "    day = df['time'].dt.floor(\"D\")\n",
    "    agg = df.groupby([day])\n",
    "    smallList = []\n",
    "    for i in agg:\n",
    "        # Take only days of which there are enough readings! 14000 is approximately 20 hours.\n",
    "        if (len(i[1]) >= 14000): \n",
    "            steps = i[1].steps\n",
    "            newsteps = steps[~np.isnan(steps)]\n",
    "            smallList.append(i[1].head(14000)) \n",
    "    subjects_data_by_day.append(smallList)\n",
    "print(subjects_data_by_day[0]) # [0] refers to data from user 0, [0][0] refers to first window of user 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many days there are for each user\n",
    "for i in range(subjects):\n",
    "    l = len(subjects_data_by_day[i])\n",
    "    print(l, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "balance = 330  # amount of days to take for every user\n",
    "identity = np.identity(subjects) # used for one hot encoding\n",
    "\n",
    "# 0. Convert dataframe to np heart rate\n",
    "subjects_data_hr = []\n",
    "# Used for experiment 3\n",
    "# for i in range(subjects):\n",
    "#     if (i == 4):\n",
    "#         smallList = []\n",
    "#         for j in range(len(subjects_data_by_day[i][:445])): # Take all possible windows for this user!\n",
    "#             smallList.append(subjects_data_by_day[i][j].hr)\n",
    "#         subjects_data_hr.append(smallList)\n",
    "#     else:\n",
    "#         smallList = []\n",
    "#         for j in range(len(subjects_data_by_day[i][:balance])):\n",
    "#             smallList.append(subjects_data_by_day[i][j].hr)\n",
    "#         subjects_data_hr.append(smallList)\n",
    "# Alternative setup\n",
    "for i in range(subjects):\n",
    "    smallList = []\n",
    "    for j in range(len(subjects_data_by_day[i][:balance])):\n",
    "        smallList.append(subjects_data_by_day[i][j].hr)\n",
    "    subjects_data_hr.append(smallList)\n",
    "\n",
    "\n",
    "# 1. Convert to train test split\n",
    "X_train_users_temp = []\n",
    "X_test_users_temp = []\n",
    "Y_train_users = []\n",
    "Y_test_users = []\n",
    "for i in range(subjects):\n",
    "    X_temp = np.array(subjects_data_hr[i])\n",
    "    Y_temp = np.full(shape=len(X_temp), fill_value=i)\n",
    "    X_train_temp, X_test_temp, Y_train_temp, Y_test_temp = train_test_split(X_temp, Y_temp, test_size=0.2, shuffle=False)\n",
    "    \n",
    "# Used in experiment 3\n",
    "#     if i == 4:\n",
    "#         X_train_sliced = X_train_temp[92:356]\n",
    "#         print(len(X_train_sliced))\n",
    "#         print(handcraft(X_train_sliced))\n",
    "#         print(handcraft(X_test_temp))\n",
    "#         X_train_users_temp.append(X_train_sliced)\n",
    "#         X_test_users_temp.append(X_test_temp) # 66 is taken anyways when making X_train \n",
    "#     else: \n",
    "#         X_train_sliced = X_train_temp[0:264] # \n",
    "#         X_train_users_temp.append(X_train_sliced)\n",
    "#         X_test_users_temp.append(X_test_temp)\n",
    "# Alternative setup\n",
    "    X_train_sliced = X_train_temp[0:264] # \n",
    "    X_train_users_temp.append(X_train_sliced)\n",
    "    X_test_users_temp.append(X_test_temp)\n",
    "    \n",
    "\n",
    "# 2. normalize on variables from train set\n",
    "mean, sd = normalize_vars(np.concatenate(np.concatenate(X_train_users_temp)))\n",
    "print(mean)\n",
    "print(sd)\n",
    "X_train_users_hr = []\n",
    "X_test_users_hr = []\n",
    "for i in range(subjects):\n",
    "    X_train_temp = np.array(normalize(X_train_users_temp[i], mean, sd))\n",
    "    X_test_temp = np.array(normalize(X_test_users_temp[i], mean, sd))\n",
    "    \n",
    "    X_train_users_hr.append(X_train_temp)\n",
    "    X_test_users_hr.append(X_test_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert windows to average age to start of test set (in days)\n",
    "s = []\n",
    "for i in range(subjects):\n",
    "    f = subjects_data_by_day[i][0].time.head(1).to_numpy()[0]\n",
    "    t = subjects_data_by_day[i][264].time.head(1).to_numpy()[0]\n",
    "    dif = int((t-f)/86400000000000) # Convert from nanoseconds to days\n",
    "    s.append(dif)\n",
    "print(np.mean(s))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert windows to age to start of test set (in days), for experiment 3\n",
    "f = subjects_data_by_day[1][0].time.head(1).to_numpy()[0]\n",
    "t = subjects_data_by_day[1][373].time.head(1).to_numpy()[0]\n",
    "dif = int((t-f)/86400000000000) # Convert from nanoseconds to days\n",
    "print(dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. Convert dataframe to np steps\n",
    "subjects_data_steps = []\n",
    "# Used for experiment 3\n",
    "# for i in range(subjects):\n",
    "#     if (i == 6):\n",
    "#         smallList = []\n",
    "#         for j in range(len(subjects_data_by_day[i][:467])):\n",
    "#             smallList.append(subjects_data_by_day[i][j].steps)\n",
    "#         subjects_data_steps.append(smallList)\n",
    "#     else:\n",
    "#         smallList = []\n",
    "#         for j in range(len(subjects_data_by_day[i][:balance])):\n",
    "#             smallList.append(subjects_data_by_day[i][j].steps)\n",
    "#         subjects_data_steps.append(smallList)\n",
    "\n",
    "# Alternative setup\n",
    "for i in range(subjects):\n",
    "    smallList = []\n",
    "    for j in range(len(subjects_data_by_day[i][:balance])):\n",
    "        smallList.append(subjects_data_by_day[i][j].steps)\n",
    "    subjects_data_steps.append(smallList)\n",
    "\n",
    "# 1. Convert to train test\n",
    "X_train_users_temp = []\n",
    "X_test_users_temp = []\n",
    "Y_train_users = []\n",
    "Y_test_users = []\n",
    "for i in range(subjects):\n",
    "    X_temp = np.array(subjects_data_steps[i])\n",
    "    Y_temp = np.full(shape=len(X_temp), fill_value=i)\n",
    "    X_train_temp, X_test_temp, Y_train_temp, Y_test_temp = train_test_split(X_temp, Y_temp, test_size=0.2, shuffle=False)\n",
    "# Used for experiment 3\n",
    "#     if i == 6:\n",
    "#         X_train_sliced = X_train_temp[109:373] # possible 352\n",
    "#         X_train_users_temp.append(X_train_sliced)\n",
    "#         X_test_users_temp.append(X_test_temp)\n",
    "#     else: \n",
    "#         X_train_sliced = X_train_temp[0:264] # \n",
    "#         X_train_users_temp.append(X_train_sliced)\n",
    "#         X_test_users_temp.append(X_test_temp)\n",
    "\n",
    "# Alternative setup\n",
    "    X_train_sliced = X_train_temp[0:264] # \n",
    "    X_train_users_temp.append(X_train_sliced)\n",
    "    X_test_users_temp.append(X_test_temp)\n",
    "\n",
    "# 3. normalize on variables from train set\n",
    "mean, sd = normalize_vars(np.concatenate(np.concatenate(X_train_users_temp)))\n",
    "print(mean)\n",
    "print(sd)\n",
    "X_train_users_steps_temp = []\n",
    "X_test_users_steps_temp = []\n",
    "for i in range(subjects):\n",
    "    X_train_temp = np.array(normalize(X_train_users_temp[i], mean, sd))\n",
    "    X_test_temp = np.array(normalize(X_test_users_temp[i], mean, sd))\n",
    "    \n",
    "    X_train_users_steps_temp.append(X_train_temp)\n",
    "    X_test_users_steps_temp.append(X_test_temp)\n",
    "\n",
    "# 1.5 convert nan to unrealistic value\n",
    "X_train_users_steps = []\n",
    "X_test_users_steps = []\n",
    "for i in range(subjects):\n",
    "    tempList = []\n",
    "    for j in range(len(X_train_users_steps_temp[i])):\n",
    "        window = X_train_users_steps_temp[i][j]\n",
    "        window = np.where(np.isnan(window), mean, window)\n",
    "        tempList.append(window)\n",
    "    X_train_users_steps.append(tempList)\n",
    "for i in range(subjects):\n",
    "    tempList = []\n",
    "    for j in range(len(X_test_users_steps_temp[i])):\n",
    "        window = X_test_users_steps_temp[i][j]\n",
    "        window = np.where(np.isnan(window), mean, window)\n",
    "        tempList.append(window)\n",
    "    X_test_users_steps.append(tempList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. apply sliding window\n",
    "X_train = []\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = [] #\n",
    "train_len = int(balance * 0.8) # 264\n",
    "test_len = int(balance * 0.2) # 66\n",
    "for j in range(train_len):\n",
    "    for i in range(subjects):\n",
    "        windows_train_hr = X_train_users_hr[i][j]\n",
    "        windows_train_steps = X_train_users_steps[i][j]\n",
    "\n",
    "        X_train.append(np.array([windows_train_hr, windows_train_steps]))\n",
    "        Y_train.append(identity[i])\n",
    "    print(j)\n",
    "print(\"done with making training windows!\")\n",
    "for j in range(test_len):\n",
    "    for i in range(subjects):\n",
    "        windows_test_hr = X_test_users_hr[i][j]\n",
    "        windows_test_steps = X_test_users_steps[i][j]\n",
    "\n",
    "        X_test.append(np.array([windows_test_hr, windows_test_steps]))\n",
    "        Y_test.append(identity[i])\n",
    "    print(j)\n",
    "print(\"done with making testing windows!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, out_channels1, out_channels2, kernel_size1, kernel_size2, stride1, stride2, lstm_hidden, lstm_layers, subjects):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(2, out_channels1, kernel_size1, stride1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(out_channels1, out_channels2, kernel_size2, stride2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lstm = nn.LSTM(input_size=864, hidden_size=lstm_hidden, num_layers=lstm_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(lstm_hidden, subjects)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = x.view(x.size(0), 1, -1)  # Reshape the tensor to have a sequence dimension\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Select the last LSTM output of the sequence\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 14000  # data for one day\n",
    "# Hyperparameters\n",
    "lr = 0.0005\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "# Device to run DNN on GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reshape the training and testing data to match the expected input shape of the model\n",
    "X_train_new = np.reshape(X_train, (len(X_train), 2, input_size))  # Should be (samples, timestamps, features)\n",
    "X_test_new = np.reshape(X_test, (len(X_test), 2, input_size))\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "\n",
    "# Model with hyperparameters included\n",
    "model = CNN_LSTM(\n",
    "    out_channels1=4, \n",
    "    out_channels2=8, \n",
    "    kernel_size1=5, \n",
    "    kernel_size2=64, \n",
    "    stride1=1, \n",
    "    stride2=32, \n",
    "    lstm_hidden=128,\n",
    "    lstm_layers=1,\n",
    "    subjects=subjects).to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "accuracy_test = []\n",
    "accuracy_train = []\n",
    "f1_score_test = []\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    correct_class_train = np.zeros(subjects)\n",
    "    total_class_train = np.zeros(subjects)\n",
    "    count_train = 0\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for i in range(0, len(X_train_new), batch_size):\n",
    "        remaining_samples = min(batch_size, len(X_train_new) - i)\n",
    "        X_batch = torch.tensor(np.array(X_train_new[i: i + remaining_samples]), dtype=torch.float32)\n",
    "        Y_batch = torch.tensor(np.array(Y_train[i: i + remaining_samples]))\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        loss = loss_fn(y_pred, Y_batch)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_correct_train = np.argmax(Y_batch.detach().cpu().numpy(), axis=1)\n",
    "        correct_class_train += np.sum(np.argmax(y_pred.detach().cpu().numpy(), axis=1) == y_correct_train)\n",
    "\n",
    "        count_train += np.sum(np.argmax(y_pred.detach().cpu().numpy(), axis=1) == y_correct_train)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    accuracy_train.append(count_train / len(X_train_new))\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_class_test = np.zeros(subjects)\n",
    "        total_class_test = np.zeros(subjects)\n",
    "        count_test = 0\n",
    "        predicted_labels = []\n",
    "        true_labels = []\n",
    "\n",
    "        for k in range(len(X_test_new)):\n",
    "            X_batch = np.reshape(X_test_new[k], (1, 2, input_size))\n",
    "            tensor = torch.tensor(X_batch, dtype=torch.float32).to(device)\n",
    "            y_pred = model(tensor)\n",
    "            y_correct = np.argmax(Y_test[k])\n",
    "            total_class_test[y_correct] += 1\n",
    "\n",
    "            if np.argmax(y_pred.detach().cpu().numpy()) == y_correct:\n",
    "                correct_class_test[y_correct] += 1\n",
    "                count_test += 1\n",
    "\n",
    "            predicted_labels.append(np.argmax(y_pred.detach().cpu().numpy()))\n",
    "            true_labels.append(y_correct)\n",
    "\n",
    "        accuracy_test.append(count_test / len(X_test_new))\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "        f1_score_test.append(f1)\n",
    "\n",
    "    final_class_test = correct_class_test / total_class_test\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "    print(f\"  Train Loss: {losses[-1]:.4f} - Train Accuracy: {accuracy_train[-1]*100:.2f}%\")\n",
    "    print(f\"  Test Accuracy: {accuracy_test[-1]*100:.2f}% - Test F1 Score: {f1_score_test[-1]:.4f}\")\n",
    "    print()\n",
    "# Output e.g. \n",
    "#  Epoch 1/100:\n",
    "#    Train Loss: 2.2145 - Train Accuracy: 21.69%\n",
    "#    Test Accuracy: 36.23% - Test F1 Score: 0.2620\n",
    "\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(18, 12))\n",
    "print(np.max(accuracy_test))\n",
    "\n",
    "# Training, and Test Accuracy plot\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(accuracy_train, label='Training Accuracy', color='blue')\n",
    "plt.plot(accuracy_test, label='Test Accuracy', color='red')\n",
    "plt.title(\"Training and Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(accuracy_test, label='Test Accuracy', color='red')\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0,1)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.legend()\n",
    "print(np.max(final_class_test[6]))\n",
    "\n",
    "# Training Loss plot\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Final Testing Accuracy per subject plot\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.bar(range(subjects), final_class_test)\n",
    "plt.title(\"Final Testing Accuracy Per Subject\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.xticks(range(subjects), range(0, subjects))  # Add class numbering\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To print the model architecture\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
